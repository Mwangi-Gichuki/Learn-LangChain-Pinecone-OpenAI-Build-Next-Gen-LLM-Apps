{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Models\n",
    "\n",
    "Starting with a simple LLM starting with GPT 3. It takes a string as input and returns a string as output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 512}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=512)\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Quantum mechanics is the scientific study of the behavior of matter and energy at the atomic and subatomic level.\n"
     ]
    }
   ],
   "source": [
    "output = llm('explain quantam mechanics in one sentence')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(llm.get_num_tokens('explain quantam mechanics in one sentence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Generation(text='\\n\\nParis', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nArea of a circle = πr², where r is the radius of the circle.', generation_info={'finish_reason': 'stop', 'logprobs': None})]]\n"
     ]
    }
   ],
   "source": [
    "output = llm.generate(['... is the capital of France.', 'What is the formula for the area of a circle?'])\n",
    "print(output.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Paris\n"
     ]
    }
   ],
   "source": [
    "print(output.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Taste the Difference at Burger Delight!\"\n"
     ]
    }
   ],
   "source": [
    "output = llm.generate(['Write an original tagline for burger restaurant'] * 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Taste the Difference at Burger Delight!\"\n",
      "\n",
      "\n",
      "\"Taste the Freshness of a Burger from Our Grill to Your Table!\"\n",
      "\n",
      "\n",
      "\"Burger Up! Deliciousness Awaits!\"\n"
     ]
    }
   ],
   "source": [
    "for o in output.generations:\n",
    "    print(o[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatModels: GPT 3.5 Turbo and GPT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5, max_tokens=1024)\n",
    "messages = [\n",
    "    SystemMessage(content='You are a pyhsicist and respond only in Kikuyu.'),\n",
    "    HumanMessage(content='Explain quantam mechanics in one sentence')\n",
    "]\n",
    "\n",
    "output = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics ni itikira ya kugundua na kuhuisha muingiliano wa vitu vidogo sana na nguvu zao, na jinsi wanavyoathiri matokeo ya ulimwengu wetu.\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language', 'virus'] template='You are an experienced virologist\\nWrite a few sentences about the following {virus} in {language}'\n"
     ]
    }
   ],
   "source": [
    "template = '''You are an experienced virologist\n",
    "Write a few sentences about the following {virus} in {language}'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=('virus', 'language'),\n",
    "    template = template\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ebola is a serious and highly contagious virus that can cause severe hemorrhagic fever in humans and other primates. It is spread through contact with bodily fluids of an infected person or animal, and can be fatal in up to 90% of cases. There is currently no known cure for Ebola, but supportive care can improve outcomes in some cases. It is important to take proper precautions when dealing with Ebola, such as wearing protective clothing and avoiding contact with the bodily fluids of infected individuals.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name='text-davinci-003', temperature=0.7)\n",
    "\n",
    "output = llm(prompt.format(virus='ebola', language='english'))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple chains\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSV ni kifupisho cha Herpes Simplex Virus ambacho ni virusi vinavyosababisha ugonjwa wa herpes. Kuna aina mbili za HSV, HSV-1 ambayo husababisha hasa vidonda vya mdomo na HSV-2 ambayo husababisha vidonda vya sehemu za siri. HSV ni moja ya virusi vinavyosambaa kwa urahisi kupitia mawasiliano ya karibu na inaweza kusababisha maumivu, vidonda, na michubuko kwenye ngozi. Ni muhimu kuchukua tahadhari za kujikinga na kuepuka kueneza HSV kwa wengine.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=('virus', 'language'),\n",
    "    template = template\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "output = chain.run({'virus': 'HSV', 'language':'Swahili'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m in Python\n",
      "\n",
      "def lin_reg(x_values, y_values):\n",
      "    \"\"\"\n",
      "    This function takes in two lists of x and y values and performs linear regression.\n",
      "    It returns the slope and intercept of the best fit line.\n",
      "    \"\"\"\n",
      "    # Calculate the mean of x and y values\n",
      "    x_mean = sum(x_values) / len(x_values) \n",
      "    y_mean = sum(y_values) / len(y_values) \n",
      "    \n",
      "    # Calculate the numerator and denominator of the slope\n",
      "    numerator = 0\n",
      "    denominator = 0\n",
      "    for i in range(len(x_values)):\n",
      "        numerator += (x_values[i] - x_mean) * (y_values[i] - y_mean)\n",
      "        denominator += (x_values[i] - x_mean) ** 2 \n",
      "    \n",
      "    # Calculate the slope and intercept of the best fit line\n",
      "    slope = numerator / denominator\n",
      "    intercept = y_mean - (slope * x_mean)\n",
      "    \n",
      "    # Return the values\n",
      "    return slope, intercept\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe function `lin_reg` takes in two lists, `x_values` and `y_values`, which represent the x and y coordinates of points on a scatter plot. It performs linear regression on these points to determine the best fit line.\n",
      "\n",
      "Here is a detailed explanation of the steps performed in the function:\n",
      "\n",
      "1. Calculate the mean of x and y values:\n",
      "   - `x_mean` is calculated by summing the values in `x_values` and dividing by the length of `x_values`.\n",
      "   - `y_mean` is calculated by summing the values in `y_values` and dividing by the length of `y_values`.\n",
      "\n",
      "2. Calculate the numerator and denominator of the slope:\n",
      "   - `numerator` is initialized as 0.\n",
      "   - `denominator` is initialized as 0.\n",
      "   - A for loop is used to iterate over the indices of `x_values`.\n",
      "   - In each iteration, the product of `(x_values[i] - x_mean)` and `(y_values[i] - y_mean)` is added to `numerator`.\n",
      "   - The square of `(x_values[i] - x_mean)` is added to `denominator`.\n",
      "\n",
      "3. Calculate the slope and intercept of the best fit line:\n",
      "   - The slope is calculated by dividing `numerator` by `denominator`.\n",
      "   - The intercept is calculated by subtracting the product of `slope` and `x_mean` from `y_mean`.\n",
      "\n",
      "4. Return the values:\n",
      "   - The function returns the slope, intercept, and a short description.\n",
      "\n",
      "Overall, the function takes in two lists of x and y values, calculates the mean of these values, calculates the numerator and denominator of the slope, and finally calculates the slope and intercept of the best fit line through linear regression. The function returns the slope, intercept, and a brief description of the functionality.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=1024)\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables =['comcept'],\n",
    "    template = '''You are an experienced scientist and Python Programmer.\n",
    "    Write a function that implements the concept of {cconcept}''' \n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt1)\n",
    "\n",
    "llm2 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables = ['function'],\n",
    "    template = 'Given the python function {function}, describe it as detailed as possible'\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm2, prompt = prompt2)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "output = overall_chain.run('linear regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given Python function `linear_regression()` is used to calculate the parameters of a linear regression line given a list of x-values and a corresponding list of y-values.\n",
      "\n",
      "The function takes in two parameters: `x_values` and `y_values`, which are lists representing the x and y values, respectively.\n",
      "\n",
      "First, the function checks if the length of the `x_values` and `y_values` lists are equal. If not, it raises a `ValueError` stating that the two lists must have the same length.\n",
      "\n",
      "Next, the function calculates the means (`x_mean` and `y_mean`) of the x and y values by summing the values in each list and dividing by the length of the list.\n",
      "\n",
      "Then, the function calculates the variances (`x_var` and `y_var`) of the x and y values. This is done by iterating over each value in the list, subtracting the mean, squaring the result, summing all the squared values, and dividing by the length of the list.\n",
      "\n",
      "After that, the function calculates the correlation coefficient (`xy_corr`) between the x and y values. This is done by iterating over pairs of x and y values and computing the product of their differences from the means, summing these products, and dividing by the length of the list.\n",
      "\n",
      "Finally, the function calculates the slope (`m`) of the regression line by dividing the correlation coefficient by the variance of the x-values. It also calculates the y-intercept (`b`) of the regression line by subtracting the product of the slope and the x-mean from the y-mean.\n",
      "\n",
      "The function then returns the slope (`m`), y-intercept (`b`), and their corresponding correlation coefficients.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I need to calculate the factorial of 20 and then take the square root\n",
      "Action: Python_REPL\n",
      "Action Input: from math import factorial; print(round(factorial(20)**0.5, 4))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m1559776268.6285\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 1559776268.6285\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1559776268.6285'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor.run('calcuate the square root of the factorial of 20 and display it with 4 decimal points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to use the power operator\n",
      "Action: Python_REPL\n",
      "Action Input: print(5.1 ** 7.3)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m146306.05007233328\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 146306.05007233328\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'146306.05007233328'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run('what is 5.1 to power 7.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
